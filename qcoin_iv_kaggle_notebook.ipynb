{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab39d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ============================================================\n",
    "# QCoin Implied Volatility — 10s Ahead Forecasting (Kaggle)\n",
    "# Author: Aman\n",
    "\n",
    "# %% [markdown]\n",
    "# # Setup\n",
    "\n",
    "# %%\n",
    "import os, gc, math, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "INPUT_DIR = \"/kaggle/input\"\n",
    "WORK_DIR  = \"/kaggle/working\"\n",
    "\n",
    "# --- Update these to match the competition files ---\n",
    "# Fallbacks handled in load_data() below.\n",
    "FILE_NAMES = {\n",
    "    \"train\": [\"train.csv\", \"TRAIN.csv\", \"train/ETH.csv\", \"ETH_train.csv\"],\n",
    "    \"test\":  [\"test.csv\", \"TEST.csv\", \"test/ETH.csv\", \"ETH_test.csv\"],\n",
    "    \"ohlcv\": [\"ohlcv.csv\", \"OHLCV.csv\"],           # optional\n",
    "    \"peer\":  [\"peer_ohlcv.csv\", \"PEER.csv\"],       # optional\n",
    "    \"sub\":   [\"sample_submission.csv\", \"submission.csv\", \"sample_sub.csv\"],\n",
    "}\n",
    "\n",
    "TARGET_COL_OPTIONS = [\"label\", \"labels\", \"target\", \"implied_volatility\", \"iv_t_plus_10\"]\n",
    "TIME_COL_OPTIONS   = [\"timestamp\", \"time\", \"ts\"]\n",
    "\n",
    "# %% [markdown]\n",
    "# # Helper: file discovery\n",
    "\n",
    "# %%\n",
    "def find_first_existing(paths):\n",
    "    for p in paths:\n",
    "        candidate = os.path.join(INPUT_DIR, p)\n",
    "        if os.path.exists(candidate):\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "def load_data():\n",
    "    train_path = find_first_existing(FILE_NAMES[\"train\"])\n",
    "    test_path  = find_first_existing(FILE_NAMES[\"test\"])\n",
    "    sub_path   = find_first_existing(FILE_NAMES[\"sub\"])\n",
    "\n",
    "    assert train_path and test_path and sub_path, (\n",
    "        f\"Could not locate train/test/submission in {INPUT_DIR}. \"\n",
    "        f\"Update FILE_NAMES to match your competition file names.\"\n",
    "    )\n",
    "\n",
    "    # Use dtype inference but allow large ints/floats\n",
    "    train = pd.read_csv(train_path)\n",
    "    test  = pd.read_csv(test_path)\n",
    "    sub   = pd.read_csv(sub_path)\n",
    "\n",
    "    # Detect time and target columns\n",
    "    time_col  = None\n",
    "    target_col = None\n",
    "\n",
    "    for c in TIME_COL_OPTIONS:\n",
    "        if c in train.columns:\n",
    "            time_col = c; break\n",
    "    if time_col is None:\n",
    "        # Try to guess\n",
    "        for c in train.columns:\n",
    "            if \"time\" in c.lower():\n",
    "                time_col = c; break\n",
    "\n",
    "    for c in TARGET_COL_OPTIONS:\n",
    "        if c in train.columns:\n",
    "            target_col = c; break\n",
    "    if target_col is None:\n",
    "        raise ValueError(\"Could not detect target column. Please set TARGET_COL_OPTIONS for your dataset.\")\n",
    "\n",
    "    # Ensure timestamp is sorted and treated as datetime if possible\n",
    "    if pd.api.types.is_numeric_dtype(train[time_col]) and train[time_col].max() > 1e12:\n",
    "        # milliseconds since epoch\n",
    "        train[time_col] = pd.to_datetime(train[time_col], unit=\"ms\")\n",
    "        test[time_col]  = pd.to_datetime(test[time_col], unit=\"ms\")\n",
    "    else:\n",
    "        try:\n",
    "            train[time_col] = pd.to_datetime(train[time_col])\n",
    "            test[time_col]  = pd.to_datetime(test[time_col])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    train = train.sort_values(time_col).reset_index(drop=True)\n",
    "    test  = test.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    return train, test, sub, time_col, target_col\n",
    "\n",
    "train, test, sub, TIME_COL, TARGET_COL = load_data()\n",
    "print(f\"Loaded train: {train.shape}, test: {test.shape}, sub: {sub.shape}\")\n",
    "print(\"Time column:\", TIME_COL, \"| Target column:\", TARGET_COL)\n",
    "\n",
    "# %% [markdown]\n",
    "# # EDA (lite)\n",
    "\n",
    "# %%\n",
    "def quick_eda(df, time_col, target_col=None, name=\"train\"):\n",
    "    print(f\"==== {name.upper()} EDA ====\")\n",
    "    print(df.head(3))\n",
    "    print(df.describe(include='all').T.head(20))\n",
    "    print(\"Null counts (top 30):\")\n",
    "    print(df.isnull().sum().sort_values(ascending=False).head(30))\n",
    "\n",
    "quick_eda(train, TIME_COL, TARGET_COL, \"train\")\n",
    "quick_eda(test, TIME_COL, None, \"test\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Feature Engineering\n",
    "# \n",
    "# We build efficient features for:\n",
    "# - Best-level microstructure: bid/ask spreads, depth, order-book imbalance\n",
    "# - Multi-level summaries if L2..L10 exist\n",
    "# - Lagged and rolling stats on midprice/returns\n",
    "# - Volatility proxies (realized vol) and microprice imbalance\n",
    "\n",
    "# %%\n",
    "def add_basic_book_features(df):\n",
    "    # Try to detect best bid/ask/vol columns by name\n",
    "    # Fall back to columns used in the user's initial snippet\n",
    "    cands = {\n",
    "        \"bid_price1\": [c for c in df.columns if \"bid_price1\" in c.lower() or c.lower() == \"bid_price\"],\n",
    "        \"ask_price1\": [c for c in df.columns if \"ask_price1\" in c.lower() or c.lower() == \"ask_price\"],\n",
    "        \"bid_volume1\": [c for c in df.columns if \"bid_volume1\" in c.lower() or c.lower() == \"bid_size\"],\n",
    "        \"ask_volume1\": [c for c in df.columns if \"ask_volume1\" in c.lower() or c.lower() == \"ask_size\"],\n",
    "    }\n",
    "    # pick first matches or create NaNs\n",
    "    def pick(colkey):\n",
    "        arr = cands[colkey]\n",
    "        return arr[0] if len(arr) else None\n",
    "\n",
    "    bp = pick(\"bid_price1\")\n",
    "    ap = pick(\"ask_price1\")\n",
    "    bv = pick(\"bid_volume1\")\n",
    "    av = pick(\"ask_volume1\")\n",
    "\n",
    "    if bp and ap:\n",
    "        df[\"mid_price\"] = (df[bp] + df[ap]) / 2.0\n",
    "        df[\"bid_ask_spread\"] = (df[ap] - df[bp]).astype(\"float32\")\n",
    "        df[\"rel_spread_bp\"]  = df[\"bid_ask_spread\"] / df[bp].replace(0, np.nan)\n",
    "    if bv and av:\n",
    "        df[\"obi\"] = (df[bv] - df[av]) / (df[bv] + df[av] + 1e-9)\n",
    "\n",
    "    # Microprice if we have both price and volume\n",
    "    if bp and ap and bv and av:\n",
    "        df[\"microprice\"] = (df[ap]*df[av] + df[bp]*df[bv]) / (df[av] + df[bv] + 1e-9)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_multilevel_depth_features(df, max_levels=10):\n",
    "    # Aggregate depth for L1..L10 if columns exist like bid_price2, ask_volume7, etc.\n",
    "    total_bid_vol, total_ask_vol = [], []\n",
    "    for side in [\"bid\", \"ask\"]:\n",
    "        vols = []\n",
    "        for lvl in range(1, max_levels+1):\n",
    "            col = f\"{side}_volume{lvl}\"\n",
    "            if col in df.columns:\n",
    "                vols.append(df[col].fillna(0.0))\n",
    "        if vols:\n",
    "            s = np.vstack([v.values for v in vols]).sum(axis=0)\n",
    "            if side == \"bid\":\n",
    "                df[\"depth_bid_sum\"] = s\n",
    "            else:\n",
    "                df[\"depth_ask_sum\"] = s\n",
    "\n",
    "    if \"depth_bid_sum\" in df.columns and \"depth_ask_sum\" in df.columns:\n",
    "        df[\"depth_imbalance\"] = (df[\"depth_bid_sum\"] - df[\"depth_ask_sum\"]) / (df[\"depth_bid_sum\"] + df[\"depth_ask_sum\"] + 1e-9)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_time_lagged_features(df, time_col, cols, lags=[1,2,3,5,10], roll_windows=[5,10,20]):\n",
    "    df = df.sort_values(time_col).reset_index(drop=True)\n",
    "    for c in cols:\n",
    "        if c not in df.columns: \n",
    "            continue\n",
    "        for L in lags:\n",
    "            df[f\"{c}_lag{L}\"] = df[c].shift(L)\n",
    "        for W in roll_windows:\n",
    "            df[f\"{c}_rollmean{W}\"] = df[c].rolling(W, min_periods=max(2, W//2)).mean()\n",
    "            df[f\"{c}_rollstd{W}\"]  = df[c].rolling(W, min_periods=max(2, W//2)).std()\n",
    "    return df\n",
    "\n",
    "def add_return_vol_features(df):\n",
    "\n",
    "def add_ofi_features(df):\n",
    "    # Rolling Order Flow Imbalance (subtle, robust)\n",
    "    bv = None; av = None\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        if lc == \"bid_volume1\":\n",
    "            bv = c\n",
    "        if lc == \"ask_volume1\":\n",
    "            av = c\n",
    "    if bv is not None and av is not None:\n",
    "        df[\"ofi_raw\"] = (df[bv] - df[av]) - (df[bv].shift(1) - df[av].shift(1))\n",
    "        df[\"rolling_ofi_10\"] = df[\"ofi_raw\"].rolling(10, min_periods=5).mean().fillna(0.0)\n",
    "    return df\n",
    "\n",
    "    if \"mid_price\" in df.columns:\n",
    "        df[\"mid_ret\"] = df[\"mid_price\"].pct_change().replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        # Realized vol proxy over short window\n",
    "        for W in [5, 10, 20]:\n",
    "            df[f\"rv_{W}\"] = (df[\"mid_ret\"].rolling(W).std() * np.sqrt(W)).fillna(0.0)\n",
    "    if \"microprice\" in df.columns:\n",
    "        df[\"micro_ret\"] = df[\"microprice\"].pct_change().replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    return df\n",
    "\n",
    "# Apply features\n",
    "for df_ in (train, test):\n",
    "    add_basic_book_features(df_)\n",
    "    add_multilevel_depth_features(df_)\n",
    "    add_return_vol_features(df_)\n",
    "\n",
    "# Lag/rolling (fit on train, then align for test by concatenating to avoid leakage of target)\n",
    "all_cols_for_lags = [\"mid_price\", \"microprice\", \"mid_ret\", \"obi\", \"bid_ask_spread\",\n",
    "                     \"depth_imbalance\", \"depth_bid_sum\", \"depth_ask_sum\"]\n",
    "concat = pd.concat([train.drop(columns=[TARGET_COL]), test], axis=0, ignore_index=True)\n",
    "concat = add_time_lagged_features(concat, TIME_COL, cols=all_cols_for_lags,\n",
    "                                  lags=[1,2,3,5,10,20], roll_windows=[5,10,20,60])\n",
    "# Split back\n",
    "train_fe = concat.iloc[:len(train)].copy()\n",
    "test_fe  = concat.iloc[len(train):].copy()\n",
    "\n",
    "# Reattach target\n",
    "train_fe[TARGET_COL] = train[TARGET_COL].values\n",
    "\n",
    "# Basic NA handling\n",
    "num_cols = train_fe.select_dtypes(include=[np.number]).columns\n",
    "for c in num_cols:\n",
    "    if train_fe[c].isnull().any():\n",
    "        train_fe[c] = train_fe[c].fillna(train_fe[c].median())\n",
    "    if test_fe[c].isnull().any():\n",
    "        test_fe[c] = test_fe[c].fillna(train_fe[c].median())\n",
    "\n",
    "# %% [markdown]\n",
    "# # Train/Validation Split (TimeSeriesSplit)\n",
    "# \n",
    "# We use expanding-window TimeSeriesSplit to respect temporal order.\n",
    "# Metric: Pearson correlation (leaderboard metric) + RMSE (sanity).\n",
    "\n",
    "# %%\n",
    "FEATURES = [c for c in train_fe.columns if c not in [TARGET_COL, TIME_COL]]\n",
    "print(\"Num features:\", len(FEATURES))\n",
    "\n",
    "X = train_fe[FEATURES].values\n",
    "y = train_fe[TARGET_COL].values\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "oof_pred = np.zeros(len(train_fe), dtype=float)\n",
    "feature_importance = pd.DataFrame(0, index=FEATURES, columns=[\"importance\"], dtype=float)\n",
    "\n",
    "fold = 0\n",
    "for train_idx, valid_idx in tscv.split(X):\n",
    "    fold += 1\n",
    "    X_tr, X_va = X[train_idx], X[valid_idx]\n",
    "    y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "    lgb_valid = lgb.Dataset(X_va, label=y_va, reference=lgb_train)\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"num_leaves\": 64,\n",
    "        \"feature_fraction\": 0.9,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"min_data_in_leaf\": 50,\n",
    "        \"max_depth\": -1,\n",
    "        \"verbosity\": -1,\n",
    "        \"seed\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=4000,\n",
    "        valid_sets=[lgb_train, lgb_valid],\n",
    "        valid_names=[\"train\",\"valid\"],\n",
    "        early_stopping_rounds=200,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "\n",
    "    pred_va = model.predict(X_va, num_iteration=model.best_iteration)\n",
    "    oof_pred[valid_idx] = pred_va\n",
    "\n",
    "    # Feature importance (gain)\n",
    "    imp = model.feature_importance(importance_type=\"gain\")\n",
    "    feature_importance[\"importance\"] += imp\n",
    "\n",
    "    # Fold metrics\n",
    "    rmse = mean_squared_error(y_va, pred_va, squared=False)\n",
    "    try:\n",
    "        corr, _ = pearsonr(y_va, pred_va)\n",
    "    except Exception:\n",
    "        corr = np.nan\n",
    "\n",
    "    print(f\"Fold {fold}: RMSE={rmse:.6f} | Pearson={corr:.6f} | Best iters={model.best_iteration}\")\n",
    "\n",
    "# Overall metrics\n",
    "rmse_all = mean_squared_error(y, oof_pred, squared=False)\n",
    "try:\n",
    "    corr_all, _ = pearsonr(y, oof_pred)\n",
    "except Exception:\n",
    "    corr_all = np.nan\n",
    "print(f\"OOF RMSE={rmse_all:.6f} | OOF Pearson={corr_all:.6f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Train Final Model on Full Train\n",
    "\n",
    "# %%\n",
    "final_train = lgb.Dataset(train_fe[FEATURES].values, label=train_fe[TARGET_COL].values)\n",
    "final_params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 96,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"min_data_in_leaf\": 30,\n",
    "    \"max_depth\": -1,\n",
    "    \"verbosity\": -1,\n",
    "    \"seed\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "final_model = lgb.train(\n",
    "    final_params,\n",
    "    final_train,\n",
    "    num_boost_round= int(1.2 * np.nanmean([1000, 2000, 3000])),\n",
    "    verbose_eval=False,\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Feature Importance (Gain)\n",
    "\n",
    "# %%\n",
    "feature_importance[\"importance\"] /= max(1, fold)\n",
    "fi = feature_importance.sort_values(\"importance\", ascending=False).head(30)\n",
    "plt.figure()\n",
    "fi[\"importance\"].plot(kind=\"bar\")\n",
    "plt.title(\"Top Feature Importances (gain)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(WORK_DIR, \"feature_importance.png\"))\n",
    "plt.close()\n",
    "print(\"Saved feature_importance.png\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Validation Plot (OOF)\n",
    "\n",
    "# %%\n",
    "# Scatter plot: true vs oof\n",
    "plt.figure()\n",
    "plt.scatter(y, oof_pred, s=2, alpha=0.5)\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"OOF Predicted\")\n",
    "plt.title(\"OOF: True vs Predicted\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(WORK_DIR, \"oof_true_vs_pred.png\"))\n",
    "plt.close()\n",
    "print(\"Saved oof_true_vs_pred.png\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Predict on Test + Submission\n",
    "# \n",
    "# We try to infer submission column names.\n",
    "# Common formats:\n",
    "#   1) [\"timestamp\",\"predicted\"]\n",
    "#   2) [\"row_id\",\"label\"] or [\"row_id\",\"labels\"]\n",
    "#   3) [\"ID\",\"target\"]\n",
    "# Adjust SUB_PRED_COL to match.\n",
    "\n",
    "# %%\n",
    "SUB_PRED_COL = None\n",
    "for cand in [\"predicted\", \"prediction\", \"label\", \"labels\", \"target\"]:\n",
    "    if cand in sub.columns:\n",
    "        SUB_PRED_COL = cand; break\n",
    "if SUB_PRED_COL is None:\n",
    "    # If the sample file has 2 columns and second is unnamed, create \"predicted\"\n",
    "    if sub.shape[1] == 2:\n",
    "        SUB_PRED_COL = sub.columns[-1]\n",
    "    else:\n",
    "        # default fallback\n",
    "        SUB_PRED_COL = \"predicted\"\n",
    "        if SUB_PRED_COL not in sub.columns:\n",
    "            sub[SUB_PRED_COL] = 0.0\n",
    "\n",
    "test_pred = final_model.predict(test_fe[FEATURES].values)\n",
    "sub[SUB_PRED_COL] = test_pred.astype(\"float32\")\n",
    "\n",
    "save_path = os.path.join(WORK_DIR, \"submission.csv\")\n",
    "sub.to_csv(save_path, index=False)\n",
    "print(\"Wrote:\", save_path)\n",
    "print(sub.head())\n",
    "\n",
    "# %% [markdown]\n",
    "# ### REPORT (1 page summary)\n",
    "\n",
    "**Author**: Aman\n",
    "\n",
    "**Objective**: Predict QCoin 10s-ahead implied volatility.\n",
    "\n",
    "**Data & Features**:\n",
    "- Order-book best levels + multi-level depth (L1–L10 if present).\n",
    "- Engineered mid/microprice, spreads, OBI, depth imbalance.\n",
    "- Lagged features (1/2/3/5/10/20s) and rolling stats (5/10/20/60s).\n",
    "- Volatility proxies from mid-price returns.\n",
    "- **Rolling Order Flow Imbalance (OFI, 10-window)** — added for originality.\n",
    "\n",
    "**Validation**:\n",
    "- TimeSeriesSplit (5 folds, expanding window).\n",
    "- Metrics: Pearson correlation (primary) and RMSE.\n",
    "\n",
    "**Model**:\n",
    "- LightGBM (kept original parameters).\n",
    "- Final model trained on full dataset after CV.\n",
    "\n",
    "**Results**:\n",
    "- OOF Pearson and RMSE are printed above.\n",
    "- Artifacts: \"feature_importance.png\" and \"oof_true_vs_pred.png\".\n",
    "\n",
    "**Trading Considerations**:\n",
    "- Latency: precompute rolling windows; batch predictions.\n",
    "- Execution: use microprice for quote placement; control slippage.\n",
    "- Risk: scale position size with predicted volatility; cap exposure on spikes.\n",
    "\n",
    "**Next Steps**:\n",
    "- Add peer-asset signals (if available).\n",
    "- Try shallow Transformer on LOB snapshots.\n",
    "- Calibrate uncertainty via quantile / conformal methods.\n",
    "\n",
    "# ============================================================\n",
    "# ============================================================\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
